ollama:
  image:
    tag: "0.1.30"
  ollama:
    gpu:
      # enable gpu if your k8s containerd/docker has gpu support
      enabled: false
    models:
      - llama2
      #- mistral
  ingress:
    enabled: true
    hosts:
    - host: ollama-api.local  # use a production hostname
      paths:
        - path: /
          pathType: Prefix

