apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-vllm
  namespace: ai-vllm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ai-vllm
  template:
    metadata:
      annotations:
      labels:
        app: ai-vllm
    spec:
      runtimeClassName: nvidia
      containers:
        - name: main
          image: vllm/vllm-openai:v0.6.3
          imagePullPolicy: IfNotPresent
          #command: ["vllm", "serve", "--port", "80", "Qwen/Qwen2.5-Coder-7B-Instruct"]
          #command: ["vllm", "serve", "--port", "80", "Qwen/Qwen2.5-Coder-1.5B-Instruct"]
          #command: ["vllm", "serve", "--port", "80", "Qwen/Qwen2.5-Coder-1.5B"]
          command: ["vllm", "serve", "--port", "80", "codellama/CodeLlama-7b-hf"]
          resources:
            limits:
              nvidia.com/gpu: 1 # requesting 1 GPU
          ports:
            - name: http
              containerPort: 80  # HTTP port
          readinessProbe:
            httpGet:
              path: /health
              port: 80
            initialDelaySeconds: 10
            periodSeconds: 30
          env:
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
            - name: PYTORCH_CUDA_ALLOC_CONF
              value: "expandable_segments:True"
            - name: TORCH_CUDA_ARCH_LIST
              value: "8.6"  # Adjust based on your GPU architecture
