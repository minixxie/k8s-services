#!/bin/bash

set -e

# First, make sure nvidia-container-runtime exists and configured to point to the k3s' containerd.sock
grep "socket-path" /etc/nvidia-container-runtime/config.toml | grep containerd.sock

grep 'default_runtime_name = "nvidia"' /etc/containerd/config.toml
grep "/usr/local/nvidia/toolkit/nvidia-container-runtime" /etc/containerd/config.toml

# vectorAdd should pass as a pre-requisite!
./vectorAdd

kubectl apply -f nvidia-smi.yaml
kubectl wait --timeout=60s --for=condition=complete job/nvidia-smi
kubectl logs $(kubectl get pod -l job-name=nvidia-smi | sed 1d | awk '{print $1}')

if [ "$(which nvidia-smi)" != "" ]; then
	echo "nvidia-smi on bare machine:"
	nvidia-smi
fi

kubectl delete -f nvidia-smi.yaml
